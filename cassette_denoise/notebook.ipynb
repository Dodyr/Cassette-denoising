{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1949b9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab3d7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend \n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "#Unet network\n",
    "def unet(pretrained_weights = None,input_size = (128,128,1)):\n",
    "    size_filter_in = 16\n",
    "    #kernel_init = 'glorot_uniform'\n",
    "    kernel_init = 'he_normal'\n",
    "    activation_layer = None \n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(inputs)\n",
    "    conv1 = LeakyReLU()(conv1)\n",
    "    conv1 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv1)\n",
    "    conv1 = LeakyReLU()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool1)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    conv2 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv2)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool2)\n",
    "    conv3 = LeakyReLU()(conv3)\n",
    "    conv3 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv3)\n",
    "    conv3 = LeakyReLU()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool3)\n",
    "    conv4 = LeakyReLU()(conv4)\n",
    "    conv4 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv4)\n",
    "    conv4 = LeakyReLU()(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(pool4)\n",
    "    conv5 = LeakyReLU()(conv5)\n",
    "    conv5 = Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv5)\n",
    "    conv5 = LeakyReLU()(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(size_filter_in*8, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6 = LeakyReLU()(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge6)\n",
    "    conv6 = LeakyReLU()(conv6)\n",
    "    conv6 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv6)\n",
    "    conv6 = LeakyReLU()(conv6)\n",
    "    up7 = Conv2D(size_filter_in*4, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7 = LeakyReLU()(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge7)\n",
    "    conv7 = LeakyReLU()(conv7)\n",
    "    conv7 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv7)\n",
    "    conv7 = LeakyReLU()(conv7)\n",
    "    up8 = Conv2D(size_filter_in*2, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8 = LeakyReLU()(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge8)\n",
    "    conv8 = LeakyReLU()(conv8)\n",
    "    conv8 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv8)\n",
    "    conv8 = LeakyReLU()(conv8)\n",
    "    \n",
    "    up9 = Conv2D(size_filter_in, 2, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9 = LeakyReLU()(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(merge9)\n",
    "    conv9 = LeakyReLU()(conv9)\n",
    "    conv9 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv9)\n",
    "    conv9 = LeakyReLU()(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = activation_layer, padding = 'same', kernel_initializer = kernel_init)(conv9)\n",
    "    conv9 = LeakyReLU()(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'tanh')(conv9)\n",
    "\n",
    "    model = Model(inputs,conv10)\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = tf.keras.losses.Huber(), metrics = ['mae'])\n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d887a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os, inspect\n",
    "import pathlib\n",
    "# ==== paths ====\n",
    "currentdir = os.getcwd()\n",
    "datadir = f\"{currentdir}/data\"\n",
    "file_no_dolby = \"ELO_1_raw.wav\"\n",
    "file_dolby    = \"ELO_1_clean.wav\"\n",
    "\n",
    "# ==== audio params ====\n",
    "sr = 44100\n",
    "\n",
    "frame_length = 32768        # ~0.74 sec\n",
    "hop_length_frame = 16384   # 50% overlap\n",
    "\n",
    "# ==== spectrogram params ====\n",
    "dim_square_spec = 128\n",
    "n_fft = 255 \n",
    "hop_length_fft = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f672c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Dolby length: 13149729\n",
      "Dolby length   : 13149729\n"
     ]
    }
   ],
   "source": [
    "y_no_dolby, sr1 = librosa.load(\n",
    "    os.path.join(datadir, file_no_dolby),\n",
    "    sr=sr,\n",
    "    mono=True\n",
    ")\n",
    "\n",
    "y_dolby, sr2 = librosa.load(\n",
    "    os.path.join(datadir, file_dolby),\n",
    "    sr=sr,\n",
    "    mono=True\n",
    ")\n",
    "\n",
    "print(\"No Dolby length:\", y_no_dolby.shape[0])\n",
    "print(\"Dolby length   :\", y_dolby.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4bc84bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 32768) (801, 32768)\n"
     ]
    }
   ],
   "source": [
    "from data_tools import audio_to_audio_frame_stack\n",
    "frames_no = audio_to_audio_frame_stack(\n",
    "    y_no_dolby, frame_length, hop_length_frame\n",
    ")\n",
    "\n",
    "frames_do = audio_to_audio_frame_stack(\n",
    "    y_dolby, frame_length, hop_length_frame\n",
    ")\n",
    "\n",
    "print(frames_no.shape, frames_do.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a067ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 128, 128) (801, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "from data_tools import numpy_audio_to_matrix_spectrogram\n",
    "X_mag_db, X_phase = numpy_audio_to_matrix_spectrogram(\n",
    "    frames_no,\n",
    "    dim_square_spec,\n",
    "    n_fft,\n",
    "    hop_length_fft\n",
    ")\n",
    "\n",
    "# Spectrograms for DOLBY (target)\n",
    "Y_mag_db, Y_phase = numpy_audio_to_matrix_spectrogram(\n",
    "    frames_do,\n",
    "    dim_square_spec,\n",
    "    n_fft,\n",
    "    hop_length_fft\n",
    ")\n",
    "\n",
    "print(X_mag_db.shape, Y_mag_db.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4f366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_in = X_mag_db\n",
    "X_ou = X_in - Y_mag_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64dddc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_tools import scaled_in, scaled_ou\n",
    "\n",
    "X_in = scaled_in(X_in)\n",
    "X_ou = scaled_ou(X_ou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad65c3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=13123584, minmax=(array([-0.68]), array([0.92000004])), mean=array([-0.03963888]), variance=array([0.10911228]), skewness=array([0.15955904]), kurtosis=array([-0.47248801]))\n",
      "DescribeResult(nobs=13123584, minmax=(array([-0.6496643]), array([0.4505511])), mean=array([-0.02981779]), variance=array([0.00171091]), skewness=array([0.6813041]), kurtosis=array([-0.36917981]))\n",
      "(801, 128, 128) (801, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "print(stats.describe(X_in.reshape(-1,1)))\n",
    "print(stats.describe(X_ou.reshape(-1,1)))\n",
    "print(X_in.shape, X_ou.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c99baf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_in = X_in.reshape(X_in.shape[0], X_in.shape[1], X_in.shape[2], 1)\n",
    "X_ou = X_ou.reshape(X_ou.shape[0], X_ou.shape[1], X_ou.shape[2], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ecd2b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_in, X_ou, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f820eecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_481081/3852312153.py:4: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_481081/3852312153.py:4: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "Devices visible to TensorFlow:\n",
      "_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 17662042371932787279)\n",
      "_DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2087586126093790765)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 19:19:09.453300: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2026-01-26 19:19:09.501410: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4199895000 Hz\n",
      "2026-01-26 19:19:09.503881: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2f45e580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2026-01-26 19:19:09.503911: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Для TF 1.x\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "print(\"Devices visible to TensorFlow:\")\n",
    "for device in sess.list_devices():\n",
    "    print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21215f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model_unet import unet\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# generator_nn = unet()  # или с предобученными весами\n",
    "# checkpoint = ModelCheckpoint('model_best.h5', monitor='val_loss', save_best_only=True)\n",
    "# history = generator_nn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=9, batch_size=80, callbacks=[checkpoint])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tf1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
